{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np\nimport torch\nimport torch.nn as nn\nfrom torchvision import datasets, transforms\nfrom torch.utils.data.sampler import SubsetRandomSampler\nimport matplotlib.pyplot as plt\nfrom torch.utils.data import ConcatDataset\nfrom PIL import Image\nimport os\nimport torchvision.models as models\nimport time\nimport copy\nimport torch.optim as optim\nfrom torch.optim import lr_scheduler\nimport torchvision.transforms as transforms\nfrom torch.utils.data import DataLoader\nfrom torchvision.datasets import ImageFolder\nimport random\nfrom collections import defaultdict\n\nimport torch.nn.functional as F\n\n\n\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\ndef get_indexes(arr, value):\n    indexes = []\n    for i in range(len(arr)):\n        if arr[i] == value:\n            indexes.append(i)\n    return indexes\n\ndef get_length_per_class(dataloader, classes):\n    class_counts = defaultdict(int)\n    total = 0\n    for batch in dataloader:\n        _, labels = batch \n        labels = labels.numpy().tolist()\n        for label in labels:\n            class_counts[label] += 1\n            total +=1\n\n    class_counts = dict(sorted(class_counts.items()))\n    for class_label, count in class_counts.items():\n        print(f\"Class {classes[class_label]}: {count} samples out of {total}\")\ndef load_data(data_dir,\n                           batch_size,\n                           data_type,\n                           noise_type,\n                           noise_percentage,                           \n                           transform,                           \n                           data_percentage=1,\n                           show_classes = False, random_seed=21):\n    \n    if noise_type == \"None\":\n        noise_type = \"\"\n        noise_percentage = \"\"\n    else:\n        noise_type = \"/\" + str(noise_type)\n        noise_percentage = \"/\" + str(noise_percentage)\n    path = data_dir + noise_type + \"/\" + data_type + noise_percentage\n    print(\"path: \", path)\n    dataset = ImageFolder(root=path, transform=transform)\n    original_classes = dataset.classes \n    num_samples = len(dataset)\n    indices = list(range(num_samples))\n\n    labels = dataset.targets\n    class_to_idx = dataset.class_to_idx\n    needed_length = int(num_samples*data_percentage/100)\n    expected_length_per_class = int(needed_length/len(original_classes))\n    print(f\"needed_length: {needed_length}, expected_length_per_class: {expected_length_per_class}\")\n    if data_percentage != 100:\n        new_indices = []\n        for key, value in class_to_idx.items():\n            all_indixes_of_class = get_indexes(labels, value)\n            new_indices.extend(all_indixes_of_class[:expected_length_per_class])\n    else:\n        new_indices = indices\n    length_dataset = len(new_indices)\n    print(\"length of final dataset:\", length_dataset)\n\n    \n    # sampler = SubsetRandomSampler(new_indices)\n\n    dataloader = DataLoader(dataset, sampler=new_indices, batch_size=batch_size)\n\n    if show_classes:\n        get_length_per_class(dataloader, original_classes)\n\n    return dataloader, length_dataset, original_classes\n\ndef train_model(model, criterion, optimizer, scheduler, num_epochs=25):\n    since = time.time()\n\n    best_model_wts = copy.deepcopy(model.state_dict())\n    best_acc = 0.0\n\n    for epoch in range(num_epochs):\n        print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n        print('-' * 10)\n\n       \n        for phase in ['train', 'val']:\n            if phase == 'train':\n                model.train()\n            else:\n                model.eval()  \n\n            running_loss = 0.0\n            running_corrects = 0\n\n            \n            for inputs, labels in dataloaders[phase]:\n                inputs =  torch.tensor(inputs).to(device)\n                labels = torch.tensor(labels).to(device)\n\n                with torch.set_grad_enabled(phase == 'train'):\n                    outputs = model(inputs)\n                    if not isinstance(outputs, torch.Tensor):\n                        outputs = outputs.logits\n                    _, preds = torch.max(outputs, 1)\n                    loss = criterion(outputs, labels)\n\n                    if phase == 'train':\n                        optimizer.zero_grad()\n                        loss.backward()\n                        optimizer.step()\n\n    \n                running_loss += loss.item() * inputs.size(0)\n                running_corrects += torch.sum(preds == labels.data)\n\n            if phase == 'train':\n                scheduler.step()\n\n            epoch_loss = running_loss / dataset_sizes[phase]\n            epoch_acc = running_corrects.double() / dataset_sizes[phase]\n\n            print('{} Loss: {:.4f} Acc: {:.4f}'.format(\n                phase, epoch_loss, epoch_acc))\n\n            # deep copy the model\n            if phase == 'val' and epoch_acc > best_acc:\n                best_acc = epoch_acc\n                best_model_wts = copy.deepcopy(model.state_dict())\n\n        print()\n\n    time_elapsed = time.time() - since\n    print('Training complete in {:.0f}m {:.0f}s'.format(\n        time_elapsed // 60, time_elapsed % 60))\n    print('Best val Acc: {:4f}'.format(best_acc))\n\n\n    model.load_state_dict(best_model_wts)\n    return model\n    \n\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-07-19T18:57:48.069635Z","iopub.execute_input":"2023-07-19T18:57:48.070012Z","iopub.status.idle":"2023-07-19T18:57:48.102475Z","shell.execute_reply.started":"2023-07-19T18:57:48.069982Z","shell.execute_reply":"2023-07-19T18:57:48.101086Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"markdown","source":"# Training alexnet","metadata":{}},{"cell_type":"code","source":"transform = transforms.Compose([\n    transforms.Resize((227, 227)),  \n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.4680, 0.4647, 0.3441], std=[0.2322, 0.2272, 0.2394]) \n])   \n\nnoise_type = \"gaussian_noise\"\nnoise_percentage = 10\ndata_percentage = 100\ntotal_size = 21000\n\ntrain_size = data_percentage*total_size/100\ndata_dir = '/kaggle/input/vegetableimages/vegetable_images'\n\ntrain_loader, train_size, classes = load_data(data_dir = data_dir,\n                           batch_size = 64,\n                           data_type = \"train\",\n                           noise_type = \"None\",\n                           noise_percentage = 0,                           \n                           transform = transform,                           \n                           data_percentage=data_percentage)\n\nvalid_loader, valid_size, _ = load_data(data_dir = data_dir,\n                           batch_size = 64,\n                           data_type = \"validation\",\n                           noise_type = \"None\",\n                           noise_percentage = 0,                           \n                           transform = transform,                           \n                           data_percentage=data_percentage)\n\nvalid_loader_with_noise, _, _ = load_data(data_dir = data_dir,\n                           batch_size = 64,\n                           data_type = \"validation\",\n                           noise_type = noise_type,\n                           noise_percentage = noise_percentage,                           \n                           transform = transform,                           \n                           data_percentage=data_percentage)\ndataloaders = {'train':  train_loader, \n               'val': valid_loader\n               }\ndataloaders_with_noise = {'train':  train_loader, \n               'val': valid_loader_with_noise\n               }\n\n\ntest_loader,test_size_, _ = load_data(data_dir = data_dir,\n                           batch_size = 64,\n                           data_type = \"test\",\n                           noise_type = \"gaussian_noise\",\n                           noise_percentage = noise_percentage,                           \n                           transform = transform,                           \n                           data_percentage=data_percentage)\n\n\ntest_loader_without_noise, _, _ = load_data(data_dir =data_dir,\n                           batch_size = 64,\n                           data_type = \"test\",\n                           noise_type = \"None\",\n                           noise_percentage = 0,                           \n                           transform = transform,                           \n                           data_percentage=data_percentage)\ndataset_sizes = {'train':  train_size, \n        'val': valid_size,\n        'test': test_size_\n       }\n\n\n","metadata":{"execution":{"iopub.status.busy":"2023-07-18T23:58:03.332993Z","iopub.execute_input":"2023-07-18T23:58:03.333521Z","iopub.status.idle":"2023-07-18T23:58:05.048927Z","shell.execute_reply.started":"2023-07-18T23:58:03.333449Z","shell.execute_reply":"2023-07-18T23:58:05.047579Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"path:  /kaggle/input/vegetableimages/vegetable_images/train\nneeded_length: 15000, expected_length_per_class: 1000\nlength of final dataset: 15000\npath:  /kaggle/input/vegetableimages/vegetable_images/validation\nneeded_length: 3000, expected_length_per_class: 200\nlength of final dataset: 3000\npath:  /kaggle/input/vegetableimages/vegetable_images/gaussian_noise/validation/10\nneeded_length: 3000, expected_length_per_class: 200\nlength of final dataset: 3000\npath:  /kaggle/input/vegetableimages/vegetable_images/gaussian_noise/test/10\nneeded_length: 3000, expected_length_per_class: 200\nlength of final dataset: 3000\npath:  /kaggle/input/vegetableimages/vegetable_images/test\nneeded_length: 3000, expected_length_per_class: 200\nlength of final dataset: 3000\n","output_type":"stream"}]},{"cell_type":"code","source":"dataset_sizes","metadata":{"execution":{"iopub.status.busy":"2023-07-18T21:35:35.373969Z","iopub.execute_input":"2023-07-18T21:35:35.374741Z","iopub.status.idle":"2023-07-18T21:35:35.382554Z","shell.execute_reply.started":"2023-07-18T21:35:35.374700Z","shell.execute_reply":"2023-07-18T21:35:35.381177Z"},"trusted":true},"execution_count":14,"outputs":[{"execution_count":14,"output_type":"execute_result","data":{"text/plain":"{'train': 15000, 'val': 3000, 'test': 3000}"},"metadata":{}}]},{"cell_type":"code","source":"alexnet = models.alexnet(pretrained=True)","metadata":{"execution":{"iopub.status.busy":"2023-07-18T21:35:36.061088Z","iopub.execute_input":"2023-07-18T21:35:36.061797Z","iopub.status.idle":"2023-07-18T21:35:36.837956Z","shell.execute_reply.started":"2023-07-18T21:35:36.061762Z","shell.execute_reply":"2023-07-18T21:35:36.836830Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"in_features = alexnet._modules['classifier'][-1].in_features\nout_features = len(classes)\nalexnet._modules['classifier'][-1] = nn.Linear(in_features, out_features, bias=True)\nprint(alexnet._modules['classifier'])","metadata":{"execution":{"iopub.status.busy":"2023-07-18T21:35:37.385593Z","iopub.execute_input":"2023-07-18T21:35:37.386018Z","iopub.status.idle":"2023-07-18T21:35:37.394290Z","shell.execute_reply.started":"2023-07-18T21:35:37.385984Z","shell.execute_reply":"2023-07-18T21:35:37.393167Z"},"trusted":true},"execution_count":16,"outputs":[{"name":"stdout","text":"Sequential(\n  (0): Dropout(p=0.5, inplace=False)\n  (1): Linear(in_features=9216, out_features=4096, bias=True)\n  (2): ReLU(inplace=True)\n  (3): Dropout(p=0.5, inplace=False)\n  (4): Linear(in_features=4096, out_features=4096, bias=True)\n  (5): ReLU(inplace=True)\n  (6): Linear(in_features=4096, out_features=15, bias=True)\n)\n","output_type":"stream"}]},{"cell_type":"code","source":"device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(device)\nalexnet = alexnet.to(device)","metadata":{"execution":{"iopub.status.busy":"2023-07-18T21:35:38.013981Z","iopub.execute_input":"2023-07-18T21:35:38.014836Z","iopub.status.idle":"2023-07-18T21:35:38.091256Z","shell.execute_reply.started":"2023-07-18T21:35:38.014790Z","shell.execute_reply":"2023-07-18T21:35:38.090134Z"},"trusted":true},"execution_count":17,"outputs":[{"name":"stdout","text":"cuda\n","output_type":"stream"}]},{"cell_type":"code","source":"F = nn.functional\ndef train_model(model, criterion, optimizer, scheduler, num_epochs=25):\n    since = time.time()\n\n    best_model_wts = copy.deepcopy(model.state_dict())\n    best_acc = 0.0\n\n    for epoch in range(num_epochs):\n        print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n        print('-' * 10)\n\n       \n        for phase in ['train', 'val']:\n            if phase == 'train':\n                model.train()\n            else:\n                model.eval()  \n\n            running_loss = 0.0\n            running_corrects = 0\n\n            \n            for inputs, labels in dataloaders[phase]:\n                inputs =  torch.tensor(inputs).to(device)\n                labels = torch.tensor(labels).to(device)\n\n                with torch.set_grad_enabled(phase == 'train'):\n                    outputs,_ = model(inputs)\n                    _, preds = torch.max(outputs, 1)\n                    loss = criterion(outputs, labels)\n\n                    if phase == 'train':\n                        optimizer.zero_grad()\n                        loss.backward()\n                        optimizer.step()\n\n    \n                running_loss += loss.item() * inputs.size(0)\n                running_corrects += torch.sum(preds == labels.data)\n\n            if phase == 'train':\n                scheduler.step()\n\n            epoch_loss = running_loss / dataset_sizes[phase]\n            epoch_acc = running_corrects.double() / dataset_sizes[phase]\n\n            print('{} Loss: {:.4f} Acc: {:.4f}'.format(\n                phase, epoch_loss, epoch_acc))\n\n            # deep copy the model\n            if phase == 'val' and epoch_acc > best_acc:\n                best_acc = epoch_acc\n                best_model_wts = copy.deepcopy(model.state_dict())\n\n        print()\n\n    time_elapsed = time.time() - since\n    print('Training complete in {:.0f}m {:.0f}s'.format(\n        time_elapsed // 60, time_elapsed % 60))\n    print('Best val Acc: {:4f}'.format(best_acc))\n\n\n    model.load_state_dict(best_model_wts)\n    return model","metadata":{"execution":{"iopub.status.busy":"2023-07-18T23:53:24.982989Z","iopub.execute_input":"2023-07-18T23:53:24.983415Z","iopub.status.idle":"2023-07-18T23:53:24.997584Z","shell.execute_reply.started":"2023-07-18T23:53:24.983380Z","shell.execute_reply":"2023-07-18T23:53:24.996360Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"for param in alexnet.parameters():\n    param.requires_grad = True","metadata":{"execution":{"iopub.status.busy":"2023-07-18T21:35:39.334282Z","iopub.execute_input":"2023-07-18T21:35:39.335060Z","iopub.status.idle":"2023-07-18T21:35:39.341253Z","shell.execute_reply.started":"2023-07-18T21:35:39.335002Z","shell.execute_reply":"2023-07-18T21:35:39.339954Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"code","source":"criterion = nn.CrossEntropyLoss()\n\n\noptimizer = optim.SGD(alexnet.parameters(), lr=0.001)\n\n\nstep_lr_scheduler = lr_scheduler.StepLR(optimizer, step_size=7, gamma=0.1)\n\nalexnet = train_model(alexnet, criterion, optimizer, step_lr_scheduler, num_epochs=15)","metadata":{"execution":{"iopub.status.busy":"2023-07-18T21:35:40.141396Z","iopub.execute_input":"2023-07-18T21:35:40.141785Z","iopub.status.idle":"2023-07-18T21:59:30.472762Z","shell.execute_reply.started":"2023-07-18T21:35:40.141755Z","shell.execute_reply":"2023-07-18T21:59:30.470514Z"},"trusted":true},"execution_count":20,"outputs":[{"name":"stdout","text":"Epoch 0/14\n----------\n","output_type":"stream"},{"name":"stderr","text":"/tmp/ipykernel_28/2272575281.py:23: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  inputs =  torch.tensor(inputs).to(device)\n/tmp/ipykernel_28/2272575281.py:24: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  labels = torch.tensor(labels).to(device)\n","output_type":"stream"},{"name":"stdout","text":"train Loss: 0.4875 Acc: 0.8967\nval Loss: 1.9262 Acc: 0.3857\n\nEpoch 1/14\n----------\ntrain Loss: 0.1976 Acc: 0.9492\nval Loss: 0.9692 Acc: 0.6803\n\nEpoch 2/14\n----------\ntrain Loss: 0.1099 Acc: 0.9717\nval Loss: 0.6022 Acc: 0.7907\n\nEpoch 3/14\n----------\ntrain Loss: 0.0778 Acc: 0.9794\nval Loss: 0.4343 Acc: 0.8557\n\nEpoch 4/14\n----------\ntrain Loss: 0.0602 Acc: 0.9824\nval Loss: 0.3366 Acc: 0.8803\n\nEpoch 5/14\n----------\ntrain Loss: 0.0499 Acc: 0.9871\nval Loss: 0.2628 Acc: 0.9100\n\nEpoch 6/14\n----------\ntrain Loss: 0.0427 Acc: 0.9881\nval Loss: 0.2289 Acc: 0.9230\n\nEpoch 7/14\n----------\ntrain Loss: 0.1049 Acc: 0.9667\nval Loss: 0.0658 Acc: 0.9793\n\nEpoch 8/14\n----------\ntrain Loss: 0.0696 Acc: 0.9794\nval Loss: 0.0576 Acc: 0.9800\n\nEpoch 9/14\n----------\ntrain Loss: 0.0645 Acc: 0.9822\nval Loss: 0.0509 Acc: 0.9827\n\nEpoch 10/14\n----------\ntrain Loss: 0.0580 Acc: 0.9829\nval Loss: 0.0467 Acc: 0.9840\n\nEpoch 11/14\n----------\ntrain Loss: 0.0545 Acc: 0.9851\nval Loss: 0.0423 Acc: 0.9863\n\nEpoch 12/14\n----------\ntrain Loss: 0.0513 Acc: 0.9862\nval Loss: 0.0403 Acc: 0.9867\n\nEpoch 13/14\n----------\ntrain Loss: 0.0503 Acc: 0.9859\nval Loss: 0.0376 Acc: 0.9883\n\nEpoch 14/14\n----------\ntrain Loss: 0.0543 Acc: 0.9843\nval Loss: 0.0357 Acc: 0.9897\n\nTraining complete in 23m 50s\nBest val Acc: 0.989667\n","output_type":"stream"}]},{"cell_type":"code","source":"torch.save(alexnet, \"alexnet_without_noise.pth\")","metadata":{"execution":{"iopub.status.busy":"2023-07-18T22:01:06.775831Z","iopub.execute_input":"2023-07-18T22:01:06.776241Z","iopub.status.idle":"2023-07-18T22:01:07.472957Z","shell.execute_reply.started":"2023-07-18T22:01:06.776200Z","shell.execute_reply":"2023-07-18T22:01:07.471252Z"},"trusted":true},"execution_count":24,"outputs":[]},{"cell_type":"markdown","source":"# Training VGG16","metadata":{}},{"cell_type":"code","source":"transform = transforms.Compose([\n    transforms.Resize(256),\n    transforms.CenterCrop(224), \n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.4680, 0.4647, 0.3441], std=[0.2322, 0.2272, 0.2394]) \n])   \n\nnoise_type = \"gaussian_noise\"\nnoise_percentage = 10\ndata_percentage = 100\ntotal_size = 21000\n\ntrain_size = data_percentage*total_size/100\ndata_dir = '/kaggle/input/vegetableimages/vegetable_images'\n\ntrain_loader, train_size, classes = load_data(data_dir = data_dir,\n                           batch_size = 64,\n                           data_type = \"train\",\n                           noise_type = \"None\",\n                           noise_percentage = 0,                           \n                           transform = transform,                           \n                           data_percentage=data_percentage)\n\nvalid_loader, valid_size, _ = load_data(data_dir = data_dir,\n                           batch_size = 64,\n                           data_type = \"validation\",\n                           noise_type = \"None\",\n                           noise_percentage = 0,                           \n                           transform = transform,                           \n                           data_percentage=data_percentage)\n\nvalid_loader_with_noise, _, _ = load_data(data_dir = data_dir,\n                           batch_size = 64,\n                           data_type = \"validation\",\n                           noise_type = noise_type,\n                           noise_percentage = noise_percentage,                           \n                           transform = transform,                           \n                           data_percentage=data_percentage)\ndataloaders = {'train':  train_loader, \n               'val': valid_loader\n               }\ndataloaders_with_noise = {'train':  train_loader, \n               'val': valid_loader_with_noise\n               }\n\n\ntest_loader,test_size_, _ = load_data(data_dir = data_dir,\n                           batch_size = 64,\n                           data_type = \"test\",\n                           noise_type = \"gaussian_noise\",\n                           noise_percentage = noise_percentage,                           \n                           transform = transform,                           \n                           data_percentage=data_percentage)\n\n\ntest_loader_without_noise, _, _ = load_data(data_dir =data_dir,\n                           batch_size = 64,\n                           data_type = \"test\",\n                           noise_type = \"None\",\n                           noise_percentage = 0,                           \n                           transform = transform,                           \n                           data_percentage=data_percentage)\ndataset_sizes = {'train':  train_size, \n        'val': valid_size,\n        'test': test_size_\n       }\n\n\n","metadata":{"execution":{"iopub.status.busy":"2023-07-19T18:57:22.683571Z","iopub.execute_input":"2023-07-19T18:57:22.684169Z","iopub.status.idle":"2023-07-19T18:57:33.908122Z","shell.execute_reply.started":"2023-07-19T18:57:22.684136Z","shell.execute_reply":"2023-07-19T18:57:33.907005Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"path:  /kaggle/input/vegetableimages/vegetable_images/train\nneeded_length: 15000, expected_length_per_class: 1000\nlength of final dataset: 15000\npath:  /kaggle/input/vegetableimages/vegetable_images/validation\nneeded_length: 3000, expected_length_per_class: 200\nlength of final dataset: 3000\npath:  /kaggle/input/vegetableimages/vegetable_images/gaussian_noise/validation/10\nneeded_length: 3000, expected_length_per_class: 200\nlength of final dataset: 3000\npath:  /kaggle/input/vegetableimages/vegetable_images/gaussian_noise/test/10\nneeded_length: 3000, expected_length_per_class: 200\nlength of final dataset: 3000\npath:  /kaggle/input/vegetableimages/vegetable_images/test\nneeded_length: 3000, expected_length_per_class: 200\nlength of final dataset: 3000\n","output_type":"stream"}]},{"cell_type":"code","source":"vgg16 = models.vgg16(pretrained=True)\nfor param in vgg16.parameters():\n    param.requires_grad = True","metadata":{"execution":{"iopub.status.busy":"2023-07-19T18:59:22.042853Z","iopub.execute_input":"2023-07-19T18:59:22.043234Z","iopub.status.idle":"2023-07-19T18:59:23.709614Z","shell.execute_reply.started":"2023-07-19T18:59:22.043185Z","shell.execute_reply":"2023-07-19T18:59:23.708582Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"in_features = vgg16._modules['classifier'][-1].in_features\nout_features = len(classes)\nvgg16._modules['classifier'][-1] = nn.Linear(in_features, out_features, bias=True)\nprint(vgg16._modules['classifier'])\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(device)\nvgg16 = vgg16.to(device)","metadata":{"execution":{"iopub.status.busy":"2023-07-19T19:01:15.538683Z","iopub.execute_input":"2023-07-19T19:01:15.539076Z","iopub.status.idle":"2023-07-19T19:01:18.956558Z","shell.execute_reply.started":"2023-07-19T19:01:15.539046Z","shell.execute_reply":"2023-07-19T19:01:18.955433Z"},"trusted":true},"execution_count":12,"outputs":[{"name":"stdout","text":"Sequential(\n  (0): Linear(in_features=25088, out_features=4096, bias=True)\n  (1): ReLU(inplace=True)\n  (2): Dropout(p=0.5, inplace=False)\n  (3): Linear(in_features=4096, out_features=4096, bias=True)\n  (4): ReLU(inplace=True)\n  (5): Dropout(p=0.5, inplace=False)\n  (6): Linear(in_features=4096, out_features=15, bias=True)\n)\ncuda\n","output_type":"stream"}]},{"cell_type":"code","source":"criterion = nn.CrossEntropyLoss()\n\n\noptimizer = optim.SGD(vgg16.parameters(), lr=0.001)\n\n\nstep_lr_scheduler = lr_scheduler.StepLR(optimizer, step_size=7, gamma=0.1)\n\nvgg16 = train_model(vgg16, criterion, optimizer, step_lr_scheduler, num_epochs=15)","metadata":{"execution":{"iopub.status.busy":"2023-07-19T19:02:42.628336Z","iopub.execute_input":"2023-07-19T19:02:42.628764Z","iopub.status.idle":"2023-07-19T20:21:06.047714Z","shell.execute_reply.started":"2023-07-19T19:02:42.628732Z","shell.execute_reply":"2023-07-19T20:21:06.046692Z"},"trusted":true},"execution_count":14,"outputs":[{"name":"stdout","text":"Epoch 0/14\n----------\n","output_type":"stream"},{"name":"stderr","text":"/tmp/ipykernel_28/1032719542.py:116: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  inputs =  torch.tensor(inputs).to(device)\n/tmp/ipykernel_28/1032719542.py:117: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  labels = torch.tensor(labels).to(device)\n","output_type":"stream"},{"name":"stdout","text":"train Loss: 0.5577 Acc: 0.8909\nval Loss: 2.1363 Acc: 0.3773\n\nEpoch 1/14\n----------\ntrain Loss: 0.1993 Acc: 0.9503\nval Loss: 1.0769 Acc: 0.6483\n\nEpoch 2/14\n----------\ntrain Loss: 0.0929 Acc: 0.9738\nval Loss: 0.5622 Acc: 0.8043\n\nEpoch 3/14\n----------\ntrain Loss: 0.0669 Acc: 0.9801\nval Loss: 0.3054 Acc: 0.8837\n\nEpoch 4/14\n----------\ntrain Loss: 0.0459 Acc: 0.9869\nval Loss: 0.1762 Acc: 0.9337\n\nEpoch 5/14\n----------\ntrain Loss: 0.0357 Acc: 0.9893\nval Loss: 0.1139 Acc: 0.9573\n\nEpoch 6/14\n----------\ntrain Loss: 0.0315 Acc: 0.9904\nval Loss: 0.1485 Acc: 0.9437\n\nEpoch 7/14\n----------\ntrain Loss: 0.0727 Acc: 0.9761\nval Loss: 0.0459 Acc: 0.9830\n\nEpoch 8/14\n----------\ntrain Loss: 0.0537 Acc: 0.9835\nval Loss: 0.0329 Acc: 0.9887\n\nEpoch 9/14\n----------\ntrain Loss: 0.0487 Acc: 0.9851\nval Loss: 0.0303 Acc: 0.9907\n\nEpoch 10/14\n----------\ntrain Loss: 0.0436 Acc: 0.9866\nval Loss: 0.0253 Acc: 0.9920\n\nEpoch 11/14\n----------\ntrain Loss: 0.0403 Acc: 0.9874\nval Loss: 0.0222 Acc: 0.9933\n\nEpoch 12/14\n----------\ntrain Loss: 0.0373 Acc: 0.9883\nval Loss: 0.0203 Acc: 0.9940\n\nEpoch 13/14\n----------\ntrain Loss: 0.0360 Acc: 0.9897\nval Loss: 0.0186 Acc: 0.9940\n\nEpoch 14/14\n----------\ntrain Loss: 0.0441 Acc: 0.9851\nval Loss: 0.0162 Acc: 0.9947\n\nTraining complete in 78m 23s\nBest val Acc: 0.994667\n","output_type":"stream"}]},{"cell_type":"code","source":"torch.save(vgg16, \"vgg16_without_noise.pth\")","metadata":{"execution":{"iopub.status.busy":"2023-07-19T20:21:58.749581Z","iopub.execute_input":"2023-07-19T20:21:58.749990Z","iopub.status.idle":"2023-07-19T20:21:59.961189Z","shell.execute_reply.started":"2023-07-19T20:21:58.749959Z","shell.execute_reply":"2023-07-19T20:21:59.960077Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}