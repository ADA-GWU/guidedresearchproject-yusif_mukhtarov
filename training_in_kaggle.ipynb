{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np\nimport torch\nimport torch.nn as nn\nfrom torchvision import datasets, transforms\nfrom torch.utils.data.sampler import SubsetRandomSampler\nimport matplotlib.pyplot as plt\nfrom torch.utils.data import ConcatDataset\nfrom PIL import Image\nimport os\nimport torchvision.models as models\nimport time\nimport copy\nimport torch.optim as optim\nfrom torch.optim import lr_scheduler\nimport torchvision.transforms as transforms\nfrom torch.utils.data import DataLoader\nfrom torchvision.datasets import ImageFolder\nimport random\nfrom collections import defaultdict\n\n\n\n\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-07-18T21:35:33.418147Z","iopub.execute_input":"2023-07-18T21:35:33.418823Z","iopub.status.idle":"2023-07-18T21:35:33.427463Z","shell.execute_reply.started":"2023-07-18T21:35:33.418786Z","shell.execute_reply":"2023-07-18T21:35:33.426175Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"def get_indexes(arr, value):\n    indexes = []\n    for i in range(len(arr)):\n        if arr[i] == value:\n            indexes.append(i)\n    return indexes\n\ndef get_length_per_class(dataloader, classes):\n    class_counts = defaultdict(int)\n    total = 0\n    for batch in dataloader:\n        _, labels = batch \n        labels = labels.numpy().tolist()\n        for label in labels:\n            class_counts[label] += 1\n            total +=1\n\n    class_counts = dict(sorted(class_counts.items()))\n    for class_label, count in class_counts.items():\n        print(f\"Class {classes[class_label]}: {count} samples out of {total}\")\ndef load_data(data_dir,\n                           batch_size,\n                           data_type,\n                           noise_type,\n                           noise_percentage,                           \n                           transform,                           \n                           data_percentage=1,\n                           show_classes = False, random_seed=21):\n    \n    if noise_type == \"None\":\n        noise_type = \"\"\n        noise_percentage = \"\"\n    else:\n        noise_type = \"/\" + str(noise_type)\n        noise_percentage = \"/\" + str(noise_percentage)\n    path = data_dir + noise_type + \"/\" + data_type + noise_percentage\n    print(\"path: \", path)\n    dataset = ImageFolder(root=path, transform=transform)\n    original_classes = dataset.classes \n    num_samples = len(dataset)\n    indices = list(range(num_samples))\n\n    labels = dataset.targets\n    class_to_idx = dataset.class_to_idx\n    needed_length = int(num_samples*data_percentage/100)\n    expected_length_per_class = int(needed_length/len(original_classes))\n    print(f\"needed_length: {needed_length}, expected_length_per_class: {expected_length_per_class}\")\n    if data_percentage != 100:\n        new_indices = []\n        for key, value in class_to_idx.items():\n            all_indixes_of_class = get_indexes(labels, value)\n            new_indices.extend(all_indixes_of_class[:expected_length_per_class])\n    else:\n        new_indices = indices\n    length_dataset = len(new_indices)\n    print(\"length of final dataset:\", length_dataset)\n\n    \n    # sampler = SubsetRandomSampler(new_indices)\n\n    dataloader = DataLoader(dataset, sampler=new_indices, batch_size=batch_size)\n\n    if show_classes:\n        get_length_per_class(dataloader, original_classes)\n\n    return dataloader, length_dataset, original_classes\n    \n\n","metadata":{"execution":{"iopub.status.busy":"2023-07-18T21:35:34.060266Z","iopub.execute_input":"2023-07-18T21:35:34.060689Z","iopub.status.idle":"2023-07-18T21:35:34.077464Z","shell.execute_reply.started":"2023-07-18T21:35:34.060638Z","shell.execute_reply":"2023-07-18T21:35:34.076229Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"transform = transforms.Compose([\n    transforms.Resize((227, 227)),  \n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.4680, 0.4647, 0.3441], std=[0.2322, 0.2272, 0.2394]) \n])   \n\nnoise_type = \"gaussian_noise\"\nnoise_percentage = 10\ndata_percentage = 100\ntotal_size = 21000\n\ntrain_size = data_percentage*total_size/100\ndata_dir = '/kaggle/input/vegetableimages/vegetable_images'\n\ntrain_loader, train_size, classes = load_data(data_dir = data_dir,\n                           batch_size = 64,\n                           data_type = \"train\",\n                           noise_type = \"None\",\n                           noise_percentage = 0,                           \n                           transform = transform,                           \n                           data_percentage=data_percentage)\n\nvalid_loader, valid_size, _ = load_data(data_dir = data_dir,\n                           batch_size = 64,\n                           data_type = \"validation\",\n                           noise_type = \"None\",\n                           noise_percentage = 0,                           \n                           transform = transform,                           \n                           data_percentage=data_percentage)\n\nvalid_loader_with_noise, _, _ = load_data(data_dir = data_dir,\n                           batch_size = 64,\n                           data_type = \"validation\",\n                           noise_type = noise_type,\n                           noise_percentage = noise_percentage,                           \n                           transform = transform,                           \n                           data_percentage=data_percentage)\ndataloaders = {'train':  train_loader, \n               'val': valid_loader\n               }\ndataloaders_with_noise = {'train':  train_loader, \n               'val': valid_loader_with_noise\n               }\n\n\ntest_loader,test_size_, _ = load_data(data_dir = data_dir,\n                           batch_size = 64,\n                           data_type = \"test\",\n                           noise_type = \"gaussian_noise\",\n                           noise_percentage = noise_percentage,                           \n                           transform = transform,                           \n                           data_percentage=data_percentage)\n\n\ntest_loader_without_noise, _, _ = load_data(data_dir =data_dir,\n                           batch_size = 64,\n                           data_type = \"test\",\n                           noise_type = \"None\",\n                           noise_percentage = 0,                           \n                           transform = transform,                           \n                           data_percentage=data_percentage)\ndataset_sizes = {'train':  train_size, \n        'val': valid_size,\n        'test': test_size_\n       }\n\n\n","metadata":{"execution":{"iopub.status.busy":"2023-07-18T21:35:34.669086Z","iopub.execute_input":"2023-07-18T21:35:34.669525Z","iopub.status.idle":"2023-07-18T21:35:34.905234Z","shell.execute_reply.started":"2023-07-18T21:35:34.669488Z","shell.execute_reply":"2023-07-18T21:35:34.904053Z"},"trusted":true},"execution_count":13,"outputs":[{"name":"stdout","text":"path:  /kaggle/input/vegetableimages/vegetable_images/train\nneeded_length: 15000, expected_length_per_class: 1000\nlength of final dataset: 15000\npath:  /kaggle/input/vegetableimages/vegetable_images/validation\nneeded_length: 3000, expected_length_per_class: 200\nlength of final dataset: 3000\npath:  /kaggle/input/vegetableimages/vegetable_images/gaussian_noise/validation/10\nneeded_length: 3000, expected_length_per_class: 200\nlength of final dataset: 3000\npath:  /kaggle/input/vegetableimages/vegetable_images/gaussian_noise/test/10\nneeded_length: 3000, expected_length_per_class: 200\nlength of final dataset: 3000\npath:  /kaggle/input/vegetableimages/vegetable_images/test\nneeded_length: 3000, expected_length_per_class: 200\nlength of final dataset: 3000\n","output_type":"stream"}]},{"cell_type":"code","source":"dataset_sizes","metadata":{"execution":{"iopub.status.busy":"2023-07-18T21:35:35.373969Z","iopub.execute_input":"2023-07-18T21:35:35.374741Z","iopub.status.idle":"2023-07-18T21:35:35.382554Z","shell.execute_reply.started":"2023-07-18T21:35:35.374700Z","shell.execute_reply":"2023-07-18T21:35:35.381177Z"},"trusted":true},"execution_count":14,"outputs":[{"execution_count":14,"output_type":"execute_result","data":{"text/plain":"{'train': 15000, 'val': 3000, 'test': 3000}"},"metadata":{}}]},{"cell_type":"code","source":"alexnet = models.alexnet(pretrained=True)","metadata":{"execution":{"iopub.status.busy":"2023-07-18T21:35:36.061088Z","iopub.execute_input":"2023-07-18T21:35:36.061797Z","iopub.status.idle":"2023-07-18T21:35:36.837956Z","shell.execute_reply.started":"2023-07-18T21:35:36.061762Z","shell.execute_reply":"2023-07-18T21:35:36.836830Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"in_features = alexnet._modules['classifier'][-1].in_features\nout_features = len(classes)\nalexnet._modules['classifier'][-1] = nn.Linear(in_features, out_features, bias=True)\nprint(alexnet._modules['classifier'])","metadata":{"execution":{"iopub.status.busy":"2023-07-18T21:35:37.385593Z","iopub.execute_input":"2023-07-18T21:35:37.386018Z","iopub.status.idle":"2023-07-18T21:35:37.394290Z","shell.execute_reply.started":"2023-07-18T21:35:37.385984Z","shell.execute_reply":"2023-07-18T21:35:37.393167Z"},"trusted":true},"execution_count":16,"outputs":[{"name":"stdout","text":"Sequential(\n  (0): Dropout(p=0.5, inplace=False)\n  (1): Linear(in_features=9216, out_features=4096, bias=True)\n  (2): ReLU(inplace=True)\n  (3): Dropout(p=0.5, inplace=False)\n  (4): Linear(in_features=4096, out_features=4096, bias=True)\n  (5): ReLU(inplace=True)\n  (6): Linear(in_features=4096, out_features=15, bias=True)\n)\n","output_type":"stream"}]},{"cell_type":"code","source":"device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(device)\nalexnet = alexnet.to(device)","metadata":{"execution":{"iopub.status.busy":"2023-07-18T21:35:38.013981Z","iopub.execute_input":"2023-07-18T21:35:38.014836Z","iopub.status.idle":"2023-07-18T21:35:38.091256Z","shell.execute_reply.started":"2023-07-18T21:35:38.014790Z","shell.execute_reply":"2023-07-18T21:35:38.090134Z"},"trusted":true},"execution_count":17,"outputs":[{"name":"stdout","text":"cuda\n","output_type":"stream"}]},{"cell_type":"code","source":"def train_model(model, criterion, optimizer, scheduler, num_epochs=25):\n    since = time.time()\n\n    best_model_wts = copy.deepcopy(model.state_dict())\n    best_acc = 0.0\n\n    for epoch in range(num_epochs):\n        print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n        print('-' * 10)\n\n       \n        for phase in ['train', 'val']:\n            if phase == 'train':\n                model.train()\n            else:\n                model.eval()  \n\n            running_loss = 0.0\n            running_corrects = 0\n\n            \n            for inputs, labels in dataloaders[phase]:\n                inputs =  torch.tensor(inputs).to(device)\n                labels = torch.tensor(labels).to(device)\n\n                with torch.set_grad_enabled(phase == 'train'):\n                    outputs = model(inputs)\n                    _, preds = torch.max(outputs, 1)\n                    loss = criterion(outputs, labels)\n\n                    if phase == 'train':\n                        optimizer.zero_grad()\n                        loss.backward()\n                        optimizer.step()\n\n    \n                running_loss += loss.item() * inputs.size(0)\n                running_corrects += torch.sum(preds == labels.data)\n\n            if phase == 'train':\n                scheduler.step()\n\n            epoch_loss = running_loss / dataset_sizes[phase]\n            epoch_acc = running_corrects.double() / dataset_sizes[phase]\n\n            print('{} Loss: {:.4f} Acc: {:.4f}'.format(\n                phase, epoch_loss, epoch_acc))\n\n            # deep copy the model\n            if phase == 'val' and epoch_acc > best_acc:\n                best_acc = epoch_acc\n                best_model_wts = copy.deepcopy(model.state_dict())\n\n        print()\n\n    time_elapsed = time.time() - since\n    print('Training complete in {:.0f}m {:.0f}s'.format(\n        time_elapsed // 60, time_elapsed % 60))\n    print('Best val Acc: {:4f}'.format(best_acc))\n\n\n    model.load_state_dict(best_model_wts)\n    return model","metadata":{"execution":{"iopub.status.busy":"2023-07-18T21:35:38.634117Z","iopub.execute_input":"2023-07-18T21:35:38.635347Z","iopub.status.idle":"2023-07-18T21:35:38.649089Z","shell.execute_reply.started":"2023-07-18T21:35:38.635297Z","shell.execute_reply":"2023-07-18T21:35:38.647999Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":"for param in alexnet.parameters():\n    param.requires_grad = True","metadata":{"execution":{"iopub.status.busy":"2023-07-18T21:35:39.334282Z","iopub.execute_input":"2023-07-18T21:35:39.335060Z","iopub.status.idle":"2023-07-18T21:35:39.341253Z","shell.execute_reply.started":"2023-07-18T21:35:39.335002Z","shell.execute_reply":"2023-07-18T21:35:39.339954Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"code","source":"criterion = nn.CrossEntropyLoss()\n\n\noptimizer = optim.SGD(alexnet.parameters(), lr=0.001)\n\n\nstep_lr_scheduler = lr_scheduler.StepLR(optimizer, step_size=7, gamma=0.1)\n\nmodel = train_model(alexnet, criterion, optimizer, step_lr_scheduler, num_epochs=15)","metadata":{"execution":{"iopub.status.busy":"2023-07-18T21:35:40.141396Z","iopub.execute_input":"2023-07-18T21:35:40.141785Z","iopub.status.idle":"2023-07-18T21:59:30.472762Z","shell.execute_reply.started":"2023-07-18T21:35:40.141755Z","shell.execute_reply":"2023-07-18T21:59:30.470514Z"},"trusted":true},"execution_count":20,"outputs":[{"name":"stdout","text":"Epoch 0/14\n----------\n","output_type":"stream"},{"name":"stderr","text":"/tmp/ipykernel_28/2272575281.py:23: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  inputs =  torch.tensor(inputs).to(device)\n/tmp/ipykernel_28/2272575281.py:24: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  labels = torch.tensor(labels).to(device)\n","output_type":"stream"},{"name":"stdout","text":"train Loss: 0.4875 Acc: 0.8967\nval Loss: 1.9262 Acc: 0.3857\n\nEpoch 1/14\n----------\ntrain Loss: 0.1976 Acc: 0.9492\nval Loss: 0.9692 Acc: 0.6803\n\nEpoch 2/14\n----------\ntrain Loss: 0.1099 Acc: 0.9717\nval Loss: 0.6022 Acc: 0.7907\n\nEpoch 3/14\n----------\ntrain Loss: 0.0778 Acc: 0.9794\nval Loss: 0.4343 Acc: 0.8557\n\nEpoch 4/14\n----------\ntrain Loss: 0.0602 Acc: 0.9824\nval Loss: 0.3366 Acc: 0.8803\n\nEpoch 5/14\n----------\ntrain Loss: 0.0499 Acc: 0.9871\nval Loss: 0.2628 Acc: 0.9100\n\nEpoch 6/14\n----------\ntrain Loss: 0.0427 Acc: 0.9881\nval Loss: 0.2289 Acc: 0.9230\n\nEpoch 7/14\n----------\ntrain Loss: 0.1049 Acc: 0.9667\nval Loss: 0.0658 Acc: 0.9793\n\nEpoch 8/14\n----------\ntrain Loss: 0.0696 Acc: 0.9794\nval Loss: 0.0576 Acc: 0.9800\n\nEpoch 9/14\n----------\ntrain Loss: 0.0645 Acc: 0.9822\nval Loss: 0.0509 Acc: 0.9827\n\nEpoch 10/14\n----------\ntrain Loss: 0.0580 Acc: 0.9829\nval Loss: 0.0467 Acc: 0.9840\n\nEpoch 11/14\n----------\ntrain Loss: 0.0545 Acc: 0.9851\nval Loss: 0.0423 Acc: 0.9863\n\nEpoch 12/14\n----------\ntrain Loss: 0.0513 Acc: 0.9862\nval Loss: 0.0403 Acc: 0.9867\n\nEpoch 13/14\n----------\ntrain Loss: 0.0503 Acc: 0.9859\nval Loss: 0.0376 Acc: 0.9883\n\nEpoch 14/14\n----------\ntrain Loss: 0.0543 Acc: 0.9843\nval Loss: 0.0357 Acc: 0.9897\n\nTraining complete in 23m 50s\nBest val Acc: 0.989667\n","output_type":"stream"}]},{"cell_type":"code","source":"torch.save(alexnet, \"alexnet_without_noise.pt\")","metadata":{"execution":{"iopub.status.busy":"2023-07-18T22:01:06.775831Z","iopub.execute_input":"2023-07-18T22:01:06.776241Z","iopub.status.idle":"2023-07-18T22:01:07.472957Z","shell.execute_reply.started":"2023-07-18T22:01:06.776200Z","shell.execute_reply":"2023-07-18T22:01:07.471252Z"},"trusted":true},"execution_count":24,"outputs":[]},{"cell_type":"code","source":"with torch.no_grad():\n    n_correct = 0\n    n_samples = 0\n    n_class_correct = [0 for i in range(len(classes))]\n    n_class_samples = [0 for i in range(len(classes))]\n    for images, labels in test_loader_without_noise:\n        images = images.to(device)\n        labels = labels.to(device)\n        outputs = model(images)\n        _, predicted = torch.max(outputs, 1)\n        n_samples += labels.size(0)\n        n_correct += (predicted == labels).sum().item()\n        \n        for i in range(len(images)):\n            label = labels[i]\n            pred = predicted[i]\n            if (label == pred):\n                n_class_correct[label] += 1\n            n_class_samples[label] += 1\n\n    acc = 100.0 * n_correct / n_samples\n    print(f'Accuracy of the network: {acc} %')\n\n    for i in range(10):\n        acc = 100.0 * n_class_correct[i] / n_class_samples[i]\n        print(f'Accuracy of {classes[i]}: {acc} %')","metadata":{"execution":{"iopub.status.busy":"2023-07-18T22:00:43.124265Z","iopub.execute_input":"2023-07-18T22:00:43.124688Z","iopub.status.idle":"2023-07-18T22:01:01.998943Z","shell.execute_reply.started":"2023-07-18T22:00:43.124654Z","shell.execute_reply":"2023-07-18T22:01:01.997171Z"},"trusted":true},"execution_count":23,"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","Cell \u001b[0;32mIn[23], line 6\u001b[0m\n\u001b[1;32m      4\u001b[0m n_class_correct \u001b[38;5;241m=\u001b[39m [\u001b[38;5;241m0\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(classes))]\n\u001b[1;32m      5\u001b[0m n_class_samples \u001b[38;5;241m=\u001b[39m [\u001b[38;5;241m0\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(classes))]\n\u001b[0;32m----> 6\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m images, labels \u001b[38;5;129;01min\u001b[39;00m test_loader_without_noise:\n\u001b[1;32m      7\u001b[0m     images \u001b[38;5;241m=\u001b[39m images\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m      8\u001b[0m     labels \u001b[38;5;241m=\u001b[39m labels\u001b[38;5;241m.\u001b[39mto(device)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:634\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    631\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    632\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    633\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 634\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    635\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    636\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    637\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    638\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:678\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    676\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    677\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 678\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    679\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    680\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py:51\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 51\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py:51\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 51\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torchvision/datasets/folder.py:229\u001b[0m, in \u001b[0;36mDatasetFolder.__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m    221\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    222\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[1;32m    223\u001b[0m \u001b[38;5;124;03m    index (int): Index\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    226\u001b[0m \u001b[38;5;124;03m    tuple: (sample, target) where target is class_index of the target class.\u001b[39;00m\n\u001b[1;32m    227\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    228\u001b[0m path, target \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msamples[index]\n\u001b[0;32m--> 229\u001b[0m sample \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    230\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransform \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    231\u001b[0m     sample \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransform(sample)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torchvision/datasets/folder.py:268\u001b[0m, in \u001b[0;36mdefault_loader\u001b[0;34m(path)\u001b[0m\n\u001b[1;32m    266\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m accimage_loader(path)\n\u001b[1;32m    267\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 268\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mpil_loader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torchvision/datasets/folder.py:247\u001b[0m, in \u001b[0;36mpil_loader\u001b[0;34m(path)\u001b[0m\n\u001b[1;32m    244\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpil_loader\u001b[39m(path: \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Image\u001b[38;5;241m.\u001b[39mImage:\n\u001b[1;32m    245\u001b[0m     \u001b[38;5;66;03m# open path as file to avoid ResourceWarning (https://github.com/python-pillow/Pillow/issues/835)\u001b[39;00m\n\u001b[1;32m    246\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(path, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[0;32m--> 247\u001b[0m         img \u001b[38;5;241m=\u001b[39m \u001b[43mImage\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    248\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m img\u001b[38;5;241m.\u001b[39mconvert(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRGB\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/PIL/Image.py:3183\u001b[0m, in \u001b[0;36mopen\u001b[0;34m(fp, mode, formats)\u001b[0m\n\u001b[1;32m   3175\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m pixels \u001b[38;5;241m>\u001b[39m MAX_IMAGE_PIXELS:\n\u001b[1;32m   3176\u001b[0m         warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m   3177\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mImage size (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpixels\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m pixels) exceeds limit of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mMAX_IMAGE_PIXELS\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m pixels, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   3178\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcould be decompression bomb DOS attack.\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   3179\u001b[0m             DecompressionBombWarning,\n\u001b[1;32m   3180\u001b[0m         )\n\u001b[0;32m-> 3183\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mopen\u001b[39m(fp, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m, formats\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m   3184\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   3185\u001b[0m \u001b[38;5;124;03m    Opens and identifies the given image file.\u001b[39;00m\n\u001b[1;32m   3186\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   3209\u001b[0m \u001b[38;5;124;03m    :exception TypeError: If ``formats`` is not ``None``, a list or a tuple.\u001b[39;00m\n\u001b[1;32m   3210\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m   3212\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m mode \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "],"ename":"KeyboardInterrupt","evalue":"","output_type":"error"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}