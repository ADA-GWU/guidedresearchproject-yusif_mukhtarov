{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ready\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import ConcatDataset\n",
    "from PIL import Image\n",
    "import os\n",
    "import torchvision.models as models\n",
    "import time\n",
    "import copy\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.datasets import ImageFolder\n",
    "import random\n",
    "from collections import defaultdict\n",
    "\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "def get_indexes(arr, value):\n",
    "    indexes = []\n",
    "    for i in range(len(arr)):\n",
    "        if arr[i] == value:\n",
    "            indexes.append(i)\n",
    "    return indexes\n",
    "\n",
    "def get_length_per_class(dataloader, classes):\n",
    "    class_counts = defaultdict(int)\n",
    "    total = 0\n",
    "    for batch in dataloader:\n",
    "        _, labels = batch \n",
    "        labels = labels.numpy().tolist()\n",
    "        for label in labels:\n",
    "            class_counts[label] += 1\n",
    "            total +=1\n",
    "\n",
    "    class_counts = dict(sorted(class_counts.items()))\n",
    "    for class_label, count in class_counts.items():\n",
    "        print(f\"Class {classes[class_label]}: {count} samples out of {total}\")\n",
    "def load_data(data_dir,\n",
    "                           batch_size,\n",
    "                           data_type,\n",
    "                           noise_type,\n",
    "                           noise_percentage,                           \n",
    "                           transform,                           \n",
    "                           data_percentage=1,\n",
    "                           show_classes = False, random_seed=21):\n",
    "    \n",
    "    if noise_type == \"None\":\n",
    "        noise_type = \"\"\n",
    "        noise_percentage = \"\"\n",
    "    else:\n",
    "        noise_type = \"/\" + str(noise_type)\n",
    "        noise_percentage = \"/\" + str(noise_percentage)\n",
    "    path = data_dir + noise_type + \"/\" + data_type + noise_percentage\n",
    "    print(\"path: \", path)\n",
    "    dataset = ImageFolder(root=path, transform=transform)\n",
    "    original_classes = dataset.classes \n",
    "    num_samples = len(dataset)\n",
    "    indices = list(range(num_samples))\n",
    "\n",
    "    labels = dataset.targets\n",
    "    class_to_idx = dataset.class_to_idx\n",
    "    needed_length = int(num_samples*data_percentage/100)\n",
    "    expected_length_per_class = int(needed_length/len(original_classes))\n",
    "    print(f\"needed_length: {needed_length}, expected_length_per_class: {expected_length_per_class}\")\n",
    "    if data_percentage != 100:\n",
    "        new_indices = []\n",
    "        for key, value in class_to_idx.items():\n",
    "            all_indixes_of_class = get_indexes(labels, value)\n",
    "            new_indices.extend(all_indixes_of_class[:expected_length_per_class])\n",
    "    else:\n",
    "        new_indices = indices\n",
    "    length_dataset = len(new_indices)\n",
    "    print(\"length of final dataset:\", length_dataset)\n",
    "\n",
    "    \n",
    "    # sampler = SubsetRandomSampler(new_indices)\n",
    "\n",
    "    dataloader = DataLoader(dataset, sampler=new_indices, batch_size=batch_size)\n",
    "\n",
    "    if show_classes:\n",
    "        get_length_per_class(dataloader, original_classes)\n",
    "        \n",
    "    random.shuffle(new_indices)\n",
    "\n",
    "   \n",
    "    dataloader = DataLoader(dataset, sampler=new_indices, batch_size=batch_size)\n",
    "\n",
    "    return dataloader, length_dataset, original_classes\n",
    "\n",
    "def testing(model):\n",
    "    with torch.no_grad():\n",
    "        n_correct = 0\n",
    "        n_samples = 0\n",
    "        n_class_correct = [0 for i in range(len(classes))]\n",
    "        n_class_samples = [0 for i in range(len(classes))]\n",
    "        for images, labels in test_loader:\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            n_samples += labels.size(0)\n",
    "            n_correct += (predicted == labels).sum().item()\n",
    "            \n",
    "            for i in range(len(images)):\n",
    "                label = labels[i]\n",
    "                pred = predicted[i]\n",
    "                if (label == pred):\n",
    "                    n_class_correct[label] += 1\n",
    "                n_class_samples[label] += 1\n",
    "\n",
    "        acc = 100.0 * n_correct / n_samples\n",
    "        print(f'Accuracy of the network: {acc} %')\n",
    "\n",
    "        for i in range(10):\n",
    "            acc = 100.0 * n_class_correct[i] / n_class_samples[i]\n",
    "            print(f'Accuracy of {classes[i]}: {acc} %')\n",
    "\n",
    "\n",
    "print('ready')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "path:  ../data/vegetable_images/train\n",
      "needed_length: 15000, expected_length_per_class: 1000\n",
      "length of final dataset: 15000\n",
      "path:  ../data/vegetable_images/validation\n",
      "needed_length: 3000, expected_length_per_class: 200\n",
      "length of final dataset: 3000\n",
      "path:  ../data/vegetable_images/gaussian_noise/validation/10\n",
      "needed_length: 3000, expected_length_per_class: 200\n",
      "length of final dataset: 3000\n",
      "path:  ../data/vegetable_images/gaussian_noise/test/10\n",
      "needed_length: 3000, expected_length_per_class: 200\n",
      "length of final dataset: 3000\n",
      "path:  ../data/vegetable_images/test\n",
      "needed_length: 3000, expected_length_per_class: 200\n",
      "length of final dataset: 3000\n"
     ]
    }
   ],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.Resize((227, 227)),  \n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.4680, 0.4647, 0.3441], std=[0.2322, 0.2272, 0.2394]) \n",
    "])   \n",
    "\n",
    "noise_type = \"gaussian_noise\"\n",
    "noise_percentage = 10\n",
    "data_percentage = 100\n",
    "total_size = 21000\n",
    "\n",
    "train_size = data_percentage*total_size/100\n",
    "data_dir = '../data/vegetable_images'\n",
    "\n",
    "train_loader, train_size, classes = load_data(data_dir = data_dir,\n",
    "                           batch_size = 64,\n",
    "                           data_type = \"train\",\n",
    "                           noise_type = \"None\",\n",
    "                           noise_percentage = 0,                           \n",
    "                           transform = transform,                           \n",
    "                           data_percentage=data_percentage)\n",
    "\n",
    "valid_loader, valid_size, _ = load_data(data_dir = data_dir,\n",
    "                           batch_size = 64,\n",
    "                           data_type = \"validation\",\n",
    "                           noise_type = \"None\",\n",
    "                           noise_percentage = 0,                           \n",
    "                           transform = transform,                           \n",
    "                           data_percentage=data_percentage)\n",
    "\n",
    "valid_loader_with_noise, _, _ = load_data(data_dir = data_dir,\n",
    "                           batch_size = 64,\n",
    "                           data_type = \"validation\",\n",
    "                           noise_type = noise_type,\n",
    "                           noise_percentage = noise_percentage,                           \n",
    "                           transform = transform,                           \n",
    "                           data_percentage=data_percentage)\n",
    "dataloaders = {'train':  train_loader, \n",
    "               'val': valid_loader\n",
    "               }\n",
    "dataloaders_with_noise = {'train':  train_loader, \n",
    "               'val': valid_loader_with_noise\n",
    "               }\n",
    "\n",
    "\n",
    "test_loader,test_size_, _ = load_data(data_dir = data_dir,\n",
    "                           batch_size = 64,\n",
    "                           data_type = \"test\",\n",
    "                           noise_type = \"gaussian_noise\",\n",
    "                           noise_percentage = noise_percentage,                           \n",
    "                           transform = transform,                           \n",
    "                           data_percentage=data_percentage)\n",
    "\n",
    "\n",
    "test_loader_without_noise, _, _ = load_data(data_dir =data_dir,\n",
    "                           batch_size = 64,\n",
    "                           data_type = \"test\",\n",
    "                           noise_type = \"None\",\n",
    "                           noise_percentage = 0,                           \n",
    "                           transform = transform,                           \n",
    "                           data_percentage=data_percentage)\n",
    "dataset_sizes = {'train':  train_size, \n",
    "        'val': valid_size,\n",
    "        'test': test_size_\n",
    "       }\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TESTING ON THE IMAGES WITH 10% NOISE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network: 96.13333333333334 %\n",
      "Accuracy of Bean: 96.5 %\n",
      "Accuracy of Bitter_Gourd: 99.0 %\n",
      "Accuracy of Bottle_Gourd: 96.0 %\n",
      "Accuracy of Brinjal: 89.0 %\n",
      "Accuracy of Broccoli: 99.5 %\n",
      "Accuracy of Cabbage: 99.0 %\n",
      "Accuracy of Capsicum: 74.5 %\n",
      "Accuracy of Carrot: 99.5 %\n",
      "Accuracy of Cauliflower: 98.0 %\n",
      "Accuracy of Cucumber: 98.5 %\n"
     ]
    }
   ],
   "source": [
    "alexnet = torch.load('../models/alexnet_without_noise.pth', map_location=torch.device('cpu'))\n",
    "testing(alexnet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network: 99.2 %\n",
      "Accuracy of Bean: 100.0 %\n",
      "Accuracy of Bitter_Gourd: 99.0 %\n",
      "Accuracy of Bottle_Gourd: 100.0 %\n",
      "Accuracy of Brinjal: 99.5 %\n",
      "Accuracy of Broccoli: 98.5 %\n",
      "Accuracy of Cabbage: 99.5 %\n",
      "Accuracy of Capsicum: 99.5 %\n",
      "Accuracy of Carrot: 98.5 %\n",
      "Accuracy of Cauliflower: 98.0 %\n",
      "Accuracy of Cucumber: 98.0 %\n"
     ]
    }
   ],
   "source": [
    "resnet = torch.load('../models/resnet_without_noise.pth', map_location=torch.device('cpu'))\n",
    "testing(resnet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network: 96.53333333333333 %\n",
      "Accuracy of Bean: 99.5 %\n",
      "Accuracy of Bitter_Gourd: 99.0 %\n",
      "Accuracy of Bottle_Gourd: 87.0 %\n",
      "Accuracy of Brinjal: 98.0 %\n",
      "Accuracy of Broccoli: 97.0 %\n",
      "Accuracy of Cabbage: 98.0 %\n",
      "Accuracy of Capsicum: 97.0 %\n",
      "Accuracy of Carrot: 97.5 %\n",
      "Accuracy of Cauliflower: 95.5 %\n",
      "Accuracy of Cucumber: 96.0 %\n"
     ]
    }
   ],
   "source": [
    "efficientnet = torch.load('../models/efficientnet_without_noise.pth', map_location=torch.device('cpu'))\n",
    "testing(efficientnet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network: 96.3 %\n",
      "Accuracy of Bean: 100.0 %\n",
      "Accuracy of Bitter_Gourd: 99.0 %\n",
      "Accuracy of Bottle_Gourd: 87.0 %\n",
      "Accuracy of Brinjal: 93.5 %\n",
      "Accuracy of Broccoli: 99.5 %\n",
      "Accuracy of Cabbage: 99.0 %\n",
      "Accuracy of Capsicum: 79.5 %\n",
      "Accuracy of Carrot: 98.5 %\n",
      "Accuracy of Cauliflower: 95.0 %\n",
      "Accuracy of Cucumber: 99.0 %\n"
     ]
    }
   ],
   "source": [
    "vgg16 = torch.load('../models/vgg16_without_noise.pth', map_location=torch.device('cpu'))\n",
    "testing(vgg16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
